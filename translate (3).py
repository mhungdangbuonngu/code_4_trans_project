# -*- coding: utf-8 -*-
"""translate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SaZLSACNIO4yfhP8l1VBxDLCpt4eKvg5
"""

import re, time
from datetime import datetime
from openai import OpenAI
import ast
from datetime import timedelta
import uvicorn

"""
# Read Data"""

def convert_to_datetime(time_strings):
    return [datetime.strptime(t, "%H:%M:%S,%f") for t in time_strings]

def datetime_to_srt_string(dt_list):
    return [dt.strftime("%H:%M:%S,%f")[:-3] for dt in dt_list]


def read_srt_file(content):
    start_times = []
    end_times = []
    texts = []

    # Split into subtitle blocks
    blocks = re.split(r'\n{2,}', content.strip())

    srt_time_pattern = re.compile(r'(\d{2}:\d{2}:\d{2},\d{3})\s-->\s(\d{2}:\d{2}:\d{2},\d{3})')

    for block in blocks:
        lines = block.strip().split('\n')

        if len(lines) < 3:
            raise ValueError(f"Block không hợp lệ: '{block}'")

        # Dòng 1: số thứ tự
        if not lines[0].isdigit():
            raise ValueError(f"Số thứ tự không hợp lệ: '{lines[0]}'")

        # Dòng 2: thời gian
        match = srt_time_pattern.match(lines[1])
        if not match:
            raise ValueError(f"Định dạng thời gian không hợp lệ: '{lines[1]}'")

        start, end = match.groups()

        # Các dòng còn lại là nội dung
        text = ' '.join(lines[2:]).strip()
        text = text.replace('"', "\'")

        start_times.append(start)
        end_times.append(end)
        texts.append(text)

    start_times, end_times = convert_to_datetime(start_times), convert_to_datetime(end_times)
    #start_times, end_times = datetime_to_srt_string(start_times), datetime_to_srt_string(end_times)
    return start_times, end_times, texts

"""# Preprocessing

## remove nonverbal lines
"""

def remove_nonverbal_lines(original_texts, start_times, end_times):
    cleaned_texts = []
    cleaned_starts = []
    cleaned_ends = []

    for text, start, end in zip(original_texts, start_times, end_times):
        text_clean = text.strip()
        # Nếu dòng chỉ là hiệu ứng âm thanh dạng [xxx] hoặc rỗng → bỏ qua
        if re.fullmatch(r"\[.*?\]", text_clean) or text_clean == "":
            continue
        cleaned_texts.append(text_clean)
        cleaned_starts.append(start)
        cleaned_ends.append(end)

    return cleaned_texts, cleaned_starts, cleaned_ends

"""## split rows"""

def split_into_sentences(text):
    pattern = r'.+?[.!?]["\')\]]*(?=\s+|$)'
    sentences = []
    last_idx = 0

    for match in re.finditer(pattern, text, flags=re.UNICODE):
        sentence = match.group().strip()
        sentences.append(sentence)
        last_idx = match.end()

    # Thêm phần còn lại nếu có (câu không có dấu kết thúc)
    remainder = text[last_idx:].strip()
    if remainder:
        sentences.append(remainder)

    return sentences

def split_rows_into_sentences(texts, starts, ends):
  original_sentences = []
  sentence_starts = []
  sentence_ends = []

  for text, s, e in zip(texts, starts, ends):
      sub_sentences = split_into_sentences(text)

      total_chars = sum(len(sent) for sent in sub_sentences)
      if total_chars == 0: continue

      total_duration = (e - s).total_seconds()
      acc_time = 0

      for sent in sub_sentences:
          ratio = len(sent) / total_chars
          seg_duration = total_duration * ratio

          seg_start = s + timedelta(seconds=acc_time)
          seg_end = seg_start + timedelta(seconds=seg_duration)

          original_sentences.append(sent)
          sentence_starts.append(seg_start)
          sentence_ends.append(seg_end)

          acc_time += seg_duration
  return original_sentences, sentence_starts, sentence_ends

"""# Create LLM Client"""

client = OpenAI(
    # base_url="http://119.17.198.75:20005/v1",
    # api_key="ioit2025",
    base_url="http://119.17.198.75:20000/v1/",
    api_key="ioit2025",
)

"""# Group rows to sentence"""

model_LLM = "/data2/Toandq/Chat_bot/model/Qwen/Qwen3-30B"

def requestLLM(model_LLM, messages):
    completion = client.chat.completions.create(
      model=model_LLM,
      messages= messages
    )

    raw=completion.choices[0].message
    match=raw.content
    if match:
        final_output = match.split("</think>")[-1].strip().replace('\\n', '')
        return final_output
    else:
        print("Không tìm thấy nội dung.")
        raise Exception("Không tìm thấy nội dung.")

def group2sentence(original_texts, model_LLM, src_lang = 'English', tgt_lang = 'Vietnamese'):
  promt = f'''You are a language assistant. I have a list of {src_lang} subtitles that were auto-generated by YouTube. Some words contain spelling or grammar mistakes, and punctuation or line breaks are not properly handled (e.g., periods or commas appear in the middle of lines without proper sentence boundaries).
  Your task is to:
  - Correct spelling, grammar, and punctuation.
  - Reformat the text so that **each sentence is placed on its own separate line**.
  - A sentence must begin with a capital letter and end with appropriate punctuation (., ?, !).
  - Merge or split lines as needed to ensure that each line contains exactly one complete sentence.
  - Do not add or remove content — preserve the original meaning faithfully.
  Here is the raw subtitle content:
  {original_texts}
  ##########
  Return the result as a Python list of strings. **Each item in the list must contain exactly one complete sentence.**'''

  messages = [
        {"role": "user", "content": "</no_think>" + promt}
    ]

  final_output = requestLLM(model_LLM, messages).strip()
  if final_output.startswith('[') and not final_output.endswith(']'):
    final_output += ']'
  #print(final_output)
  sentences_list = ast.literal_eval(final_output)
  return sentences_list

"""## timestamp for sentences"""

from difflib import SequenceMatcher

def best_match_window(target, original_texts, start_idx, max_window=6, threshold=0.9):
    """
    Tìm đoạn con tốt nhất từ original_texts[start_idx:] sao cho khớp với target
    """
    best_ratio = 0
    best_end = start_idx + 1
    n = len(original_texts)

    for window in range(1, max_window + 1):
        end_idx = start_idx + window
        if end_idx > n:
            break
        segment = ' '.join(original_texts[start_idx:end_idx])
        ratio = SequenceMatcher(None, segment.strip(), target.strip()).ratio()
        if ratio > best_ratio:
            best_ratio = ratio
            best_end = end_idx
        if ratio == 1.0:
            break  # khớp tuyệt đối rồi

    if best_ratio >= threshold:
        return best_end
    return None  # không tìm được đoạn đủ tốt

def align_sentences_sliding_window(sentences_list, original_texts, start_times, end_times, max_window=6, threshold=0.85):
    aligned_sentences = []
    aligned_start_times = []
    aligned_end_times = []

    i = 0  # pointer trong original_texts

    for sent in sentences_list:
        best_end = best_match_window(sent, original_texts, i, max_window=max_window, threshold=threshold)
        if best_end is None:
            # Không khớp được, bỏ qua hoặc xử lý đặc biệt
            print(f"⚠️ Không khớp được: {sent[:40]}...")
            continue

        aligned_sentences.append(sent)
        aligned_start_times.append(start_times[i])
        aligned_end_times.append(end_times[best_end - 1])
        i = best_end  # dịch con trỏ tới đoạn chưa dùng

    return aligned_start_times, aligned_end_times, aligned_sentences

"""# Translate"""

def trans_list(sentences_list, src_lang, tgt_lang):
  if src_lang == tgt_lang:
    return sentences_list
  else: return trans_llm(sentences_list, src_lang, tgt_lang, model_LLM)

def trans_llm(sentences_list, src_lang, tgt_lang, model_LLM):
  promt = f'''You are a professional subtitle translator.

I have a list of {src_lang} sentences that were extracted from subtitles.
Please translate each sentence into **natural, fluent {tgt_lang}** suitable for subtitle display.

Requirements:
- Preserve the meaning, tone, and context of the original sentence.
- Keep translations concise and appropriate for subtitles.
- Return the result as a valid **Python list of strings**.

Input:
{sentences_list}

##########
Return:
Return the result as a valid **Python list of strings**.'''

  messages = [
        {"role": "user", "content": "</no_think>" + promt}
    ]
  final_output = requestLLM(model_LLM, messages).strip()
  if final_output.startswith('[') and not final_output.endswith(']'):
    final_output += ']'
  #print(final_output)
  sentences_list = ast.literal_eval(final_output)
  return sentences_list

"""## split rows"""

def split_sentences_to_subtitles(sentences_list, model_LLM, language='Vietnamese'):
  promt = f'''You are a subtitle splitting assistant.
Your job is to break long {language} sentences into short chunks that can be used as subtitles for on-screen display.
Guidelines:
- Break long sentences into multiple chunks if necessary.
- Each chunk should be short enough to display clearly as a subtitle (ideally no more than 12–15 words).
- Preserve punctuation, grammar, and natural breaks where possible (e.g., at commas or conjunctions).
- Do not merge sentences. Each sentence should be processed independently.
- The output should be a list of lists as a valid **Python list of strings**— each sublist contains the chunks for one sentence, in order.

Example:
Input:
[
"Two people have died and 559 have been arrested during Champions League final celebrations in Paris.",
"Nikki, bring us up to date on what the French authorities are saying.",
"This comes after Paris Sanjgeran won their first ever European Champions League football final, soundly beating Inter Milan 5-nil in Munich.",
"Thời tiết ngày mai tại các tỉnh miền Bắc sẽ có nắng nóng gay gắt, đặc biệt là khu vực trung du và đồng bằng.",
"Chúng tôi biết rằng khoảng 5.400 sĩ quan cảnh sát đã được triển khai để đề phòng bất kỳ sự cố nào vào tối qua.",
]
'Chúng tôi biết rằng khoảng 5.400 sĩ quan cảnh sát',
 '',
 'đã được triển khai để đề phòng bất kỳ sự cố nào',
 '',
 '',
 'vào tối qua.',
Output:
[
  ["Two people have died and 559 have been arrested", "during Champions League final celebrations in Paris."],
  ["Nikki, bring us up to date on what the French authorities are saying."],
  ["This comes after Paris Sanjgeran won their first ever", "European Champions League football final,", "soundly beating Inter Milan 5-nil in Munich."],
  ["Thời tiết ngày mai tại các tỉnh miền Bắc", "sẽ có nắng nóng gay gắt,", "đặc biệt là khu vực trung du và đồng bằng."],
  ["Chúng tôi biết rằng khoảng 5.400 sĩ quan cảnh sát đã được triển khai", "để đề phòng bất kỳ sự cố nào vào tối qua."],
]

Now process this input:

{sentences_list}
##########
Return only the list of subtitle chunks as a valid **Python list of strings**, in JSON format.'''

  messages = [
        {"role": "user", "content": "</no_think>" + promt}
    ]

  final_output = requestLLM(model_LLM, messages).strip()
  if final_output.startswith('[') and not final_output.endswith(']'):
    final_output += ']'
  #print(final_output)
  subtitles_list = ast.literal_eval(final_output)
  return subtitles_list

def split_sentences_to_subtitles(sentences_list, model_LLM, tgt_lang='English'):
  promt = f'''You are a subtitle splitting assistant.
Your job is to break long {tgt_lang} sentences into short chunks that can be used as subtitles for on-screen display.
Guidelines:
- Each input sentence should be split into one or more subtitle chunks.
- Each chunk must contain no more than **12 words**.
- Always **preserve punctuation marks** (e.g., `.`, `,`, `!`, `?`) at the **end of each chunk**, especially if they appear in the original sentence.
- Break at natural language boundaries where possible: punctuation, conjunctions (and, but, so), or clause ends.
- Do **not merge multiple sentences** together.
- Do **not remove or modify punctuation** from the original sentence.
- Each sentence must be split **independently**.
- Output must be a list of lists as a valid **Python list of strings**, where each inner list contains subtitle chunks for one sentence.
- Make sure the final punctuation (like periods or commas) is preserved in the output chunk exactly as in the input.

Example:
Input:
[
  "Two people have died and 559 have been arrested during Champions League final celebrations in Paris.",
  "Nikki, bring us up to date on what the French authorities are saying.",
  "This comes after Paris Sanjgeran won their first ever European Champions League football final, soundly beating Inter Milan 5-nil in Munich.",
  "Thời tiết ngày mai tại các tỉnh miền Bắc sẽ có nắng nóng gay gắt, đặc biệt là khu vực trung du và đồng bằng.",
  "Chúng tôi biết rằng khoảng 5.400 sĩ quan cảnh sát đã được triển khai để đề phòng bất kỳ sự cố nào vào tối qua.",
  "Chúng tôi cũng đã thấy một số xe hơi bị đốt cháy tại Paris và Bộ Nội vụ cung cấp số liệu cho biết có 692 vụ cháy trong đó có 264 vụ ở xe cộ.",
  "Bây giờ, hai người đã chết, các cơ quan chức năng cho biết đó là một cậu bé 17 tuổi bị đâm vào ngực tại thị trấn Dax trong một trong những cuộc ăn mừng đó.",
  "Vì vậy, những người chức trách, như tôi đã nói, đã bắt giữ một số người, 559 người bị bắt giữ trong các sự việc tại Paris.",
  "Bộ Nội vụ Samantha đã xác nhận rằng hai người đã thiệt mạng và 192 người khác bị thương trong những cuộc ăn mừng đó như bạn đã nói, sau khi Paris Sanjaman giành chức vô địch trận chung kết cúp châu Âu tại Munich.",
]

Output:
[
  ["Two people have died and 559 have been arrested" , "during Champions League final celebrations in Paris."],
  ["Nikki, bring us up to date" , "on what the French authorities are saying."],
  ["This comes after" , "Paris Sanjgeran won their first ever" , "European Champions League football final," , "soundly beating Inter Milan 5-nil in Munich."],
  ["Thời tiết ngày mai tại các tỉnh miền Bắc sẽ có nắng nóng gay gắt," , "đặc biệt là khu vực trung du và đồng bằng."],
  ["Chúng tôi biết rằng khoảng 5.400 sĩ quan cảnh sát đã được triển khai" , "để đề phòng bất kỳ sự cố nào vào tối qua."],
  ["Chúng tôi cũng đã thấy một số xe hơi bị đốt cháy tại Paris" , "và Bộ Nội vụ cung cấp số liệu cho biết có 692 vụ cháy" , "trong đó có 264 vụ ở xe cộ."],
  ["Bây giờ, hai người đã chết," , "các cơ quan chức năng cho biết" , "đó là một cậu bé 17 tuổi bị đâm vào ngực tại thị trấn Dax" , "trong một trong những cuộc ăn mừng đó."],
  ["Vì vậy, những người chức trách, như tôi đã nói," , "đã bắt giữ một số người," , "559 người bị bắt giữ trong các sự việc tại Paris."],
  ["Bộ Nội vụ Samantha đã xác nhận rằng" , "hai người đã thiệt mạng và 192 người khác bị thương" , "trong những cuộc ăn mừng đó như bạn đã nói," , "sau khi Paris Sanjaman giành chức vô địch" , "trận chung kết cúp châu Âu tại Munich."],
]

Now process this input:

{sentences_list}
##########
Return only the list of subtitle chunks as a valid **Python list of strings**, in JSON format.'''

  messages = [
        {"role": "user", "content": "</no_think>" + promt}
    ]

  final_output = requestLLM(model_LLM, messages).strip()
  if final_output.startswith('[') and not final_output.endswith(']'):
    final_output += ']'
  #print(final_output)
  subtitles_list = ast.literal_eval(final_output)
  return subtitles_list

def split_sentence_into_chunks2(sentence, max_words=15):
    words = sentence.strip().split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(current_chunk) >= max_words or re.search(r'[.,!?]$', word):
            chunks.append(' '.join(current_chunk).strip())
            current_chunk = []

    # Thêm phần còn lại nếu có
    if current_chunk:
        chunks.append(' '.join(current_chunk).strip())

    return chunks

def split_sentences_to_subtitles2(sentences, max_words=15):
    return [split_sentence_into_chunks2(s, max_words=max_words) for s in sentences]

"""## timestamp chunks"""

def compute_chunk_timestamps(chunked_sentences, aligned_starts, aligned_ends):
    chunk_timestamps = []

    for sentence_chunks, start_time, end_time in zip(chunked_sentences, aligned_starts, aligned_ends):
        total_chars = sum(len(chunk) for chunk in sentence_chunks)
        sentence_duration = end_time - start_time
        current_time = start_time

        sentence_chunk_times = []

        for chunk in sentence_chunks:
            chunk_ratio = len(chunk) / total_chars if total_chars > 0 else 0
            chunk_duration = chunk_ratio * sentence_duration
            chunk_start = current_time
            chunk_end = chunk_start + chunk_duration

            sentence_chunk_times.append((chunk_start, chunk_end))
            current_time = chunk_end

        chunk_timestamps.append(sentence_chunk_times)

    return chunk_timestamps

def group_short_chunk(subtitles_list, chunk_timestamps):
  for sentence_chunks, timestamp_chunks in zip(subtitles_list, chunk_timestamps):
    i = 0
    while i < len(sentence_chunks) - 1:
      start_time, end_time = timestamp_chunks[i]
      if (end_time - start_time).total_seconds() < 1.0 and i + 1 < len(sentence_chunks):
        sentence_chunks[i + 1] = sentence_chunks[i] + ' ' + sentence_chunks[i + 1]
        timestamp_chunks[i + 1] = (timestamp_chunks[i][0], timestamp_chunks[i + 1][1])
        del sentence_chunks[i]
        del timestamp_chunks[i]
      else:
        i += 1

x = [1,2,3]
del x[1]
x

"""# Generate srt"""

BASE_TIME = datetime(1900, 1, 1)

def format_srt_time(dt):
    delta = dt - BASE_TIME
    total_seconds = delta.total_seconds()
    hours = int(total_seconds // 3600)
    minutes = int((total_seconds % 3600) // 60)
    seconds = int(total_seconds % 60)
    millis = int((total_seconds - int(total_seconds)) * 1000)
    return f"{hours:02}:{minutes:02}:{seconds:02},{millis:03}"

def generate_srt_from_chunks(subtitles_list, chunk_timestamps):
    srt_lines = []
    idx = 1

    for chunks, timestamps in zip(subtitles_list, chunk_timestamps):
        for chunk, (start_time, end_time) in zip(chunks, timestamps):
            start_str = format_srt_time(start_time)
            end_str = format_srt_time(end_time)
            srt_lines.append(f"{idx}")
            srt_lines.append(f"{start_str} --> {end_str}")
            srt_lines.append(chunk)
            srt_lines.append("")  # Blank line
            idx += 1

    return "\n".join(srt_lines)

"""# Export API"""

from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.responses import JSONResponse
import json
from fastapi.responses import ORJSONResponse, HTMLResponse
from fastapi.middleware.cors import CORSMiddleware


def open_cors(app):
    origins = ["http://localhost:1630", "http://127.0.0.1:1630", "*"]
    app.add_middleware(
        CORSMiddleware,
        allow_origins=origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
#------------------------------
# # create app
app = FastAPI(default_response_class=ORJSONResponse)
open_cors(app)

# Define request schema
class SubtitleRequest(BaseModel):
    subtitle: str
    source_lang: str = "English"
    target_lang: str = "Vietnamese"
    max_words_per_line: int = 15
    #llm_model: str = "nnlb"
    #translate: bool = True
    #split_sentences: bool = True

# Dummy process function – replace with your actual logic
def translate_subtitles(req: SubtitleRequest):
  subtitle_contents, source_lang, target_lang, max_words_per_line = req.subtitle, req.source_lang, req.target_lang, req.max_words_per_line

  start_times, end_times, original_texts = read_srt_file(subtitle_contents)
  texts_clean, starts_clean, ends_clean = remove_nonverbal_lines(original_texts, start_times, end_times)
  original_sentences, sentence_starts, sentence_ends = split_rows_into_sentences(texts_clean, starts_clean, ends_clean)
  sentences_list = group2sentence(original_sentences, model_LLM, src_lang=source_lang, tgt_lang=target_lang)
  aligned_starts, aligned_ends, aligned_texts = align_sentences_sliding_window(
      sentences_list,
      original_sentences,
      sentence_starts,
      sentence_ends,
      max_window=10,
      threshold=0.85
  )
  assert len(aligned_texts) == len(sentences_list)
  trans_sentences_list = trans_list(sentences_list, src_lang=source_lang, tgt_lang=target_lang)
  subtitles_list = split_sentences_to_subtitles(trans_sentences_list, model_LLM, tgt_lang=target_lang)
  chunk_timestamps = compute_chunk_timestamps(subtitles_list, aligned_starts, aligned_ends)
  group_short_chunk(subtitles_list, chunk_timestamps)
  srt_text = generate_srt_from_chunks(subtitles_list, chunk_timestamps)

  origin_idx = 0
  origin_srt_lines = []
  idx = 0
  srt_lines = []

  for chunks, timestamps in zip(subtitles_list, chunk_timestamps):
      for chunk, (start_time, end_time) in zip(chunks, timestamps):
          srt_lines.append(chunk)
          idx += 1
          update_origin = False
          while True:
            if origin_idx < len(sentence_starts) and ((sentence_ends[origin_idx] - end_time).total_seconds() < 0.01 or (sentence_starts[origin_idx] - start_time).total_seconds() < 0.01):
              if update_origin:
                srt_lines.append('')
                idx += 1
              update_origin = True
              origin_srt_lines.append(original_sentences[origin_idx])
              origin_idx += 1
            else:
              if not update_origin:
                origin_srt_lines.append('')
              break
  assert len(origin_srt_lines) == len(srt_lines)
  return {
    'srt_file_data': srt_text,
    'source_data': origin_srt_lines,
    'translate_data': srt_lines,
  }

@app.post("/api/translate_subtitles")
async def api_translate_subtitles(payload: SubtitleRequest):
  tik = time.time()
  try:
    result = translate_subtitles(payload)
  except Exception as ex:
    result = {'_exception':str(ex)}
  if '_exception' not in result:
    result['_exception'] = None
  result['_time'] = time.time() - tik
  return JSONResponse(content=result, media_type="application/json")
  #return JSONResponse(content=json.dumps(result, ensure_ascii=False, indent=2))

"""# Run"""

# run app
def run(host=None, logLevel='info'):
    if not host: host = '0.0.0.0:1630'
    host = host.split(':')

    host[1] = int(host[1])

    try:
        print('Uvicorn start.')
        ##kill_socket(host[1])
        ##call_proc(host, logLevel)
        uvicorn.run(app=app, host=host[0], port=host[1], workers=0, limit_concurrency = 20000,
            reload=False, limit_max_requests = 20000, log_level=logLevel)
        #uvicorn.run("main:app", host=host[0], port=host[1], workers=0, reload=True, limit_concurrency = 20000,
        #            limit_max_requests = 20000, log_level=logLevel)
        print('Uvicorn end.')
    except Exception as ex:
        print('Uvicorn expception: ')
        #print('%s -> %s' % (str(type(ex))[8:-2], traceback.format_exc()))

def main():
  #subtile_path = '/content/drive/MyDrive/Colab/VTV/translate/data/Dự báo thời tiết 19h45 - 06_06_2025 _ Gió mùa Tây Nam hoạt động mạnh _ VTVWDB.srt'
  subtile_path = '/content/drive/MyDrive/Colab/VTV/translate/data/Two dead and hundreds arrested in France after PSG Champions League win _ BBC News.srt'

  with open(subtile_path, 'r', encoding='utf-8') as file:
      subtitle_contents = file.read()

  start_times, end_times, original_texts = read_srt_file(subtitle_contents)
  texts_clean, starts_clean, ends_clean = remove_nonverbal_lines(original_texts, start_times, end_times)
  original_sentences, sentence_starts, sentence_ends = split_rows_into_sentences(texts_clean, starts_clean, ends_clean)
  sentences_list = group2sentence(original_sentences, model_LLM)
  aligned_starts, aligned_ends, aligned_texts = align_sentences_sliding_window(
      sentences_list,
      original_sentences,
      sentence_starts,
      sentence_ends,
      max_window=10,
      threshold=0.85
  )
  assert len(aligned_texts) == len(sentences_list)
  trans_sentences_list = trans_list(sentences_list, src_lang='English', tgt_lang='Vietnamese')
  #subtitles_list = split_sentences_to_subtitles2(trans_sentences_list, max_words=15)
  subtitles_list = split_sentences_to_subtitles(trans_sentences_list, model_LLM)
  chunk_timestamps = compute_chunk_timestamps(subtitles_list, aligned_starts, aligned_ends)
  group_short_chunk(subtitles_list, chunk_timestamps)
  srt_text = generate_srt_from_chunks(subtitles_list, chunk_timestamps)

  with open("output.srt", "w", encoding="utf-8") as f:
      f.write(srt_text)

if __name__ == "__main__":
    #import sys
    #args = sys.argv[1:]
    #run(args[0] if len(args) > 0 else None)
    run()